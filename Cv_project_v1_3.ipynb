{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cv -project-v1_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BishoyAbd/CV-projec-v1_1/blob/master/Cv_project_v1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch\n",
        "!pip install gdown"
      ],
      "metadata": {
        "id": "7T0_QChMcvtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d09de35-c3b8-4c10-d1a3-f6ed84617de0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.12.0+cu113)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (4.1.1)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.11.0+cu113)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "!ls \"drive/My Drive/cv project files/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwaYQnuGiUmD",
        "outputId": "9fcbe163-928a-43f8-a4c5-3701ba03c252"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "emos2.mp4  emos.mp4  EMOTION_DETECTOR_1.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download video from drive\n",
        "!gdown --id 1RzZ9k_aYPYsY6kn0EB7luGuUliGnaJyL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C4eZdn7THQk",
        "outputId": "67597db8-c517-4cb3-a61e-e1fec2150c6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RzZ9k_aYPYsY6kn0EB7luGuUliGnaJyL\n",
            "To: /content/videoplayback.mp4\n",
            "100% 429M/429M [00:04<00:00, 98.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "MqHaqxgecI8K"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch as pt\n",
        "from torchvision import transforms as trans\n",
        "from torch import nn\n",
        "from imutils.video import FileVideoStream\n",
        "import cv2\n",
        "import time\n",
        "import glob\n",
        "device= 'cuda' if pt.cuda.is_available() else 'cpu'\n",
        "# print(device)\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VID_PATH=\"/content/videoplayback.mp4\"\n",
        "#initialize the model, selecting GPU as device and keeping all faces\n",
        "mtcnn = MTCNN(select_largest=False, image_size=48, device='cuda', keep_all=False,post_process=False)\n",
        "\n",
        "#Load the video\n",
        "v_cap = cv2.VideoCapture(VID_PATH)\n",
        "v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = int(v_cap.get(cv2.CAP_PROP_FPS))\n",
        "timestamps = [] #list for the timestamps\n",
        "\n",
        "# Loop through video\n",
        "batch_size = 256\n",
        "batch_num = 0\n",
        "frames = []\n",
        "boxes = []\n",
        "landmarks = []\n",
        "view_frames = []\n",
        "view_boxes = []\n",
        "view_landmarks = []\n",
        "sf = 1 # get one frame per second\n",
        "\n",
        "for i in range(v_len):\n",
        "    # Load frame\n",
        "    success, frame = v_cap.read()\n",
        "    #select every n frames\n",
        "    if i % math.floor(fps)*sf == 0: #skip to get every sf seconds\n",
        "        success, frame = v_cap.retrieve()\n",
        "    else:\n",
        "        continue\n",
        "    if not success:\n",
        "        print(\"ERROR\")\n",
        "        continue\n",
        "\n",
        "    # Add to batch, resizing for speed\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = Image.fromarray(frame)\n",
        "    frame = frame.resize([int(f * 0.25) for f in frame.size])\n",
        "    frames.append(frame)\n",
        "    timestamps.append(math.floor(v_cap.get(cv2.CAP_PROP_POS_MSEC))//1000) #append timestamp of the frame\n",
        "    # print(math.floor(v_cap.get(cv2.CAP_PROP_POS_MSEC))*1000)\n",
        "    \n",
        "    # When batch is full, detect faces and reset batch list\n",
        "    if len(frames) >= batch_size:\n",
        "        #print(\"Executed MTCNN batch num\" + str(batch_num))\n",
        "        #define save path \n",
        "        save_paths = [f'drive/My Drive/cv project files/faces/{timestamps[i]}.jpg' for i in range(len(frames))]\n",
        "        #save all faces to files\n",
        "        mtcnn(frames, save_path=save_paths)\n",
        "        #increase batch number\n",
        "        batch_num += 1\n",
        "        frames = [] #clear frame list\n",
        "        timestamps = [] #clear timestamplist\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48F6Q5O5cyou",
        "outputId": "810baf6a-f79b-4da6-baba-6e8f2915ef7e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
            "/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/utils/detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
            "/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  boxes = np.array(boxes)\n",
            "/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  points = np.array(points)\n",
            "/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/mtcnn.py:444: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  selected_boxes = np.array(selected_boxes)\n",
            "/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/mtcnn.py:446: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  selected_points = np.array(selected_points)\n",
            "/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  probs = np.array(probs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_stamps=[]\n",
        "filelist = glob.glob('drive/My Drive/cv project files/faces/*')\n",
        "for f in filelist:\n",
        "  time_stamp=int(os.path.split(f)[1].split('.')[0])\n",
        "  time_stamps.append(time_stamp)\n",
        "np_time_stamps=np.array(time_stamps)\n",
        "\n",
        "np_images=np.array([Image.open(f) for f in filelist])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkmN1y6Ddsdk",
        "outputId": "d675febb-82ec-4089-811f-b0a0325b9fe4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: FutureWarning: The input object of type 'JpegImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'JpegImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frame=cv2.imread(\"drive/My Drive/cv project files/sf.\")\n",
        "# frame = Image.fromarray(frame)\n",
        "# frame = frame.resize([int(f * 0.25) for f in frame.size])\n",
        "# faces=mtcnn(frame)\n",
        "\n",
        "# type(faces)\n",
        "# # fig, axes = plt.subplots(1, len(faces))\n",
        "# # for face, ax in zip(faces, axes):\n",
        "# #     ax.imshow(face.permute(1, 2, 0).int().numpy())\n",
        "# #     ax.axis('off')\n",
        "# # fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT2X9jf1KhKD",
        "outputId": "a9e11301-15b8-4413-94ce-39298b7e5525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDetector(nn.Module):\n",
        "  def __init__(self, in_ch,classes):\n",
        "    super(EmotionDetector,self).__init__()\n",
        "\n",
        "    self.conv1=nn.Conv2d(in_channels=in_ch,out_channels=60, kernel_size=(3,3))\n",
        "    self.relu1=nn.ReLU()\n",
        "    self.bn1=nn.BatchNorm2d(60)\n",
        "    self.maxPool1=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "\n",
        "    self.conv2=nn.Conv2d(in_channels=60,out_channels=120, kernel_size=(3,3))\n",
        "    self.relu2=nn.ReLU()\n",
        "    self.bn2=nn.BatchNorm2d(120)\n",
        "    self.maxPool2=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "\n",
        "    self.conv3=nn.Conv2d(in_channels=120,out_channels=240, kernel_size=(3,3))\n",
        "    self.relu3=nn.ReLU()\n",
        "    self.bn3=nn.BatchNorm2d(240)\n",
        "    # self.maxPool3=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "    #28*28*1 -> 27-13-> 12-6 *80 -> 5*4*100\n",
        "    #    \n",
        "    self.dropout=nn.Dropout2d()\n",
        "\n",
        "    self.conv4=nn.Conv2d(in_channels=240,out_channels=480, kernel_size=(3,3))\n",
        "    self.relu4=nn.ReLU()\n",
        "    self.maxPool4=nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "\n",
        "\n",
        "    # self.conv5=nn.Conv2d(in_channels=120,out_channels=150, kernel_size=(5,5))\n",
        "    # self.relu5=nn.ReLU()\n",
        "    # self.maxPool5=nn.MaxPool2d(kernel_size=(2,2), stride=(1,1))\n",
        "\n",
        "\n",
        "    self.fc1=nn.Linear(in_features=480*3*3,out_features=200)\n",
        "    self.relu_fc1=nn.ReLU()\n",
        "\n",
        "    self.fc2=nn.Linear(in_features=200,out_features=classes)\n",
        "    self.lsm=nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    debug=False\n",
        "    x=x.float()\n",
        "    if debug : print(\"input to cv1 \",x.shape)\n",
        "    x=self.conv1(x)\n",
        "    if debug : print(\"input to mxp1\",x.shape)\n",
        "    x=self.bn1(x)\n",
        "    x=self.relu1(x)\n",
        "    x=self.maxPool1(x)\n",
        "    # x=self.dropout(x)\n",
        "\n",
        "\n",
        "    if debug :print(\"input to cv2\",x.shape)\n",
        "    x=self.conv2(x)\n",
        "    x=self.bn2(x)\n",
        "\n",
        "    if debug : print(\"input to mxp2\",x.shape)\n",
        "    x=self.relu2(x)\n",
        "    x=self.maxPool2(x)\n",
        "    # x=self.dropout(x)\n",
        "\n",
        "    if debug : print(\"input to cv3\",x.shape)\n",
        "    x=self.conv3(x)\n",
        "    x=self.bn3(x)\n",
        "    x=self.relu3(x)\n",
        "    if debug : print(\"input to mxp3\",x.shape)\n",
        "    # x=self.maxPool3(x)\n",
        "    x=self.dropout(x)\n",
        "\n",
        "    \n",
        "    x=self.conv4(x)\n",
        "    x=self.relu4(x)\n",
        "    x=self.maxPool4(x)\n",
        "    x=self.dropout(x)\n",
        "    # x=self.conv5(x)\n",
        "    # x=self.relu5(x)\n",
        "    # x=self.maxPool5(x)\n",
        "\n",
        "    if debug : print(\"input to fc1\",x.shape)\n",
        "    x = pt.flatten(x,1)\n",
        "    \n",
        "    x=self.fc1(x)\n",
        "    x=self.relu_fc1(x)\n",
        "    if debug : print(\"input to fc2 \",x.shape)\n",
        "    x=self.fc2(x)\n",
        "    if debug : print(\"input to lsm \",x.shape)\n",
        "    output=self.lsm(x)\n",
        "    if debug : print(\"final output \",x.shape)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "yEs6Y2Yyks2k"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME= 'EMOTION_DETECTOR_1.pt'\n",
        "MODEL_PATH = f\"drive/My Drive/cv project files//{MODEL_NAME}\" \n",
        "CLASSES=3\n",
        "model=EmotionDetector(in_ch=1,classes=CLASSES).to(device)\n",
        "model.load_state_dict(pt.load(MODEL_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I2r4px6lCLs",
        "outputId": "b9387976-d762-4a54-a254-1beabad1e7ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IfVHPVholSp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(np_images))\n",
        "output_emo=[]\n",
        "model.eval()\n",
        "with pt.no_grad():\n",
        "  for image, time in zip(np_images,np_time_stamps):\n",
        "      image2 = Image.fromarray(np.uint8(image)).convert('L')\n",
        "      img = trans.ToTensor()(image2).type(pt.FloatTensor).to(\"cuda\").reshape(1,1,48,48)\n",
        "      pred=pt.exp(model(img))\n",
        "      idx=pred.argmax(axis=1).cpu().numpy()[0]\n",
        "      output_emo.append(idx)"
      ],
      "metadata": {
        "id": "WSeUxe6AktbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce83f85a-19d5-4a21-d6b7-8a8b0dcd2a0c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_emo[1:50])\n",
        "print(time_stamps[1:50])\n",
        "np_emo=np.array(output_emo)\n",
        "np_ts=np.array(time_stamps)"
      ],
      "metadata": {
        "id": "QGShfM8ccsG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b29f50-2952-4bad-c151-1cf3454f5339"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 1, 1, 0, 2, 2, 2, 0, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i =0\n",
        "#every 1 min get mode\n",
        "step =60\n",
        "#new time stamps every element represent a minute\n",
        "ticks=[]\n",
        "emotion_min=[]\n",
        "min=0\n",
        "while i +step < len(np_emo):\n",
        "  #dont panic, jsut calculating the mode,\n",
        "  #returns a tuple of lists get first e from tuple then first from list\n",
        "  emotion_min.append(stats.mode(np_emo[i:i+step])[0][0])\n",
        "  ticks.append(min)\n",
        "  min+=1\n",
        "  i=i+step\n"
      ],
      "metadata": {
        "id": "TSjjTgRZscYn"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame()\n",
        "df['emotion']=emotion_min\n",
        "df['time_Stamps']=ticks\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PYCVUgyBoOI1",
        "outputId": "1cf1e49f-0ce7-449d-d635-b5860f2903a3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   emotion  time_Stamps\n",
              "0        2            0\n",
              "1        2            1\n",
              "2        2            2\n",
              "3        2            3\n",
              "4        2            4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7570e0c4-4704-4a8e-bc92-192fd2b62533\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>time_Stamps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7570e0c4-4704-4a8e-bc92-192fd2b62533')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7570e0c4-4704-4a8e-bc92-192fd2b62533 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7570e0c4-4704-4a8e-bc92-192fd2b62533');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(16,5)})\n",
        "sns.histplot(data=df, x='time_Stamps', hue='emotion', multiple='fill')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "u2RLxJhGoLW2",
        "outputId": "eb3ba794-1875-4700-f201-44bd0dc8a429"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1fb2dc6510>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}